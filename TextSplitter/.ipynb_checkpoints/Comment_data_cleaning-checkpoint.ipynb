{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "94b86f30-1c07-40ef-81bc-c148cd796c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_text_splitter import TextSplitter\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import PyPDF2\n",
    "import os\n",
    "import math\n",
    "import textwrap\n",
    "from typing import List, Optional\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import wordninja\n",
    "\n",
    "from io import StringIO\n",
    "\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "# If using the modern openai client:\n",
    "# pip install openai\n",
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f853c96-bba0-4340-9b5e-d493fb23c2ce",
   "metadata": {},
   "source": [
    "## Set Directory for Comment Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "d84fde23-57de-45f4-b8e7-c7aa942fad7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163\n"
     ]
    }
   ],
   "source": [
    "dir = r\"C:/Users/Eric.Englin/DOT OST\\Volpe-Group-JPODataProgram - ROADII/Lab/Use Cases/Public Comments/attachments\"\n",
    "all_entries = os.listdir(dir)\n",
    "print(len(all_entries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "446a58ae-1b64-4bf4-b13d-27439e414dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pdf_to_string(file_path):\n",
    "    file_path = dir + \"/\" + file_path\n",
    "    output_string = StringIO()\n",
    "    with open(file_path, 'rb') as in_file:\n",
    "        parser = PDFParser(in_file)\n",
    "        doc = PDFDocument(parser)\n",
    "        rsrcmgr = PDFResourceManager()\n",
    "        device = TextConverter(rsrcmgr, output_string, laparams=LAParams())\n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "        for page in PDFPage.create_pages(doc):\n",
    "            interpreter.process_page(page)\n",
    "\n",
    "    return(output_string.getvalue())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3973787c-6cc9-4687-9f14-4185eb2cb092",
   "metadata": {},
   "source": [
    "## Deterministic Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "06d41e7d-d538-48da-8dcc-fc4f4d24eaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_by_size(this_attachment_text):\n",
    "    document_list = []\n",
    "    #content_list = []\n",
    "    chunks = []\n",
    "    # Sentence-aware splitting: split into sentences then pack\n",
    "    sentence_end_re = re.compile(r'(?<=[.!?])\\s+')\n",
    "    # Keep original paragraphs roughly: split into sentences, but first normalize whitespace\n",
    "    sentences = sentence_end_re.split(this_attachment_text)\n",
    "    cur_len = 0\n",
    "    cur = []\n",
    "    for s in sentences:\n",
    "        s = wordninja.split(s)\n",
    "        # Join back with spaces\n",
    "        s = \" \".join(s)\n",
    "      #  print(s)\n",
    "\n",
    "        if cur_len + len(s) > 800:\n",
    "            cur.append(s)\n",
    "            document_list.append(x)\n",
    "            chunks.append(\" \".join(cur))\n",
    "            cur_len = 0       \n",
    "            #print(len(document_list), len(chunks))\n",
    "           # print(cur)\n",
    "            cur = []\n",
    "        else:\n",
    "            cur.append(s)\n",
    "            cur_len += len(s) \n",
    "\n",
    "    chunk_df = pd.DataFrame({\n",
    "        \"Document\":document_list,\n",
    "        \"Chunk\": chunks\n",
    "    })\n",
    "    return chunk_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd669ca5-fd0c-4af1-81b0-24bfbda4ae0d",
   "metadata": {},
   "source": [
    "## Extract Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "8b16e07b-903d-4fd8-92b1-21288dad93c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_df = pd.DataFrame(columns=['Document', 'Chunk'])\n",
    "content_list = []\n",
    "for x in all_entries[0:3]:\n",
    "   # document_list.append(x)\n",
    "    this_attachment_text = convert_pdf_to_string(x)\n",
    "    this_attachment_text = re.sub(r'\\n', ' ', this_attachment_text)\n",
    "    content_list.append(this_attachment_text)\n",
    "    this_chunk_df = chunk_by_size(this_attachment_text)\n",
    "    chunk_df = pd.concat([chunk_df, this_chunk_df])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "d68c874d-d96f-489e-a645-c8498812a9d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(169, 2)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "79af13ec-d2e7-4de1-9dee-c01d28fe53f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Chunk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EPA-HQ-OAR-2025-0194-0094_attachment_1.pdf</td>\n",
       "      <td>Research A Section 508 conform ant HTML versio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EPA-HQ-OAR-2025-0194-0094_attachment_1.pdf</td>\n",
       "      <td>OBJECTIVE The aim of this study was to investi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EPA-HQ-OAR-2025-0194-0094_attachment_1.pdf</td>\n",
       "      <td>After adjusting for lifestyle and medical hist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EPA-HQ-OAR-2025-0194-0094_attachment_1.pdf</td>\n",
       "      <td>This n ding could be use ful for creating a po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EPA-HQ-OAR-2025-0194-0094_attachment_1.pdf</td>\n",
       "      <td>To meet the goals of the Paris Agreement a red...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Document  \\\n",
       "0  EPA-HQ-OAR-2025-0194-0094_attachment_1.pdf   \n",
       "1  EPA-HQ-OAR-2025-0194-0094_attachment_1.pdf   \n",
       "2  EPA-HQ-OAR-2025-0194-0094_attachment_1.pdf   \n",
       "3  EPA-HQ-OAR-2025-0194-0094_attachment_1.pdf   \n",
       "4  EPA-HQ-OAR-2025-0194-0094_attachment_1.pdf   \n",
       "\n",
       "                                               Chunk  \n",
       "0  Research A Section 508 conform ant HTML versio...  \n",
       "1  OBJECTIVE The aim of this study was to investi...  \n",
       "2  After adjusting for lifestyle and medical hist...  \n",
       "3  This n ding could be use ful for creating a po...  \n",
       "4  To meet the goals of the Paris Agreement a red...  "
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "fcbbc548-12b3-452f-92b4-5bb340e85c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_df.to_excel(\"chunks_attempt.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbfa664-948e-4a2d-81ae-a1081c7fb755",
   "metadata": {},
   "source": [
    "## OpenAI API "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2eb675f-ec62-42e9-971c-b9443711e5bb",
   "metadata": {},
   "source": [
    "#### OpenAI Setup and Quick Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "fe74ed1d-1643-415c-b51e-678d26a902d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./../openai_key.txt', 'r') as file:\n",
    "    OPENAI_API_KEY = file.read()\n",
    "   # print(OPENAI_API_KEY)\n",
    "\n",
    "openai.api_key = OPENAI_API_KEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "315fec2f-4799-4e83-8531-3e088d1d8c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Your test request has been received. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "client = openai.OpenAI(\n",
    "    api_key=openai.api_key,\n",
    "    base_url=\"http://10.75.42.137:4000/\" \n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"GPT-4.1-nano\", # model to send to the proxy\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"this is a test request\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2ec7a2-b075-4d5e-841d-412191114ba9",
   "metadata": {},
   "source": [
    "#### Method 1: Relies entirely on OpenAI to Chunk Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "b67d1c4e-f60e-4f8a-bf59-b6b8c4ed6630",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = content_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "c32607d9-a414-4d16-80e0-1e4c3d85fce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = f\"\"\"\n",
    "    Instructions:\n",
    "    - Split the entire text into smaller comments/chunks appropriate for posting as separate comments.\n",
    "    - Aim for roughly {desired_chunk_chars} characters per chunk, but respect sentence boundaries and coherence.\n",
    "    - If the text contains lists, code blocks, or special sections, keep them intact where possible.\n",
    "    - Output must be a valid JSON array of strings, e.g. [\"chunk1\", \"chunk2\", ...].\n",
    "    - Do not include any extra prose or explanation.\n",
    "    - Clean up text where words may be spaces may be incorrectly missing or needed\n",
    "    - Clean up text where words may be mispelled\n",
    "    - Make sure to do the entire document not just the first page\n",
    "    - The length of the final output must be roughly similar to the length of the input text\n",
    "\n",
    "    Text:\n",
    "    \\\"\\\"\\\"\n",
    "    {text}\n",
    "    \\\"\\\"\\\"\n",
    "\n",
    "    \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "14b48e7a-fcfc-4323-849c-973a5f8e02c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = client.chat.completions.create(\n",
    "    model=\"GPT-4.1-nano\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that splits a long comment or text into smaller, \"\n",
    "    \"readable comment-sized chunks. Each chunk should be a coherent piece of text (complete sentences if possible), \"\n",
    "    f\"roughly around {desired_chunk_chars} characters (give or take). \"\n",
    "    \"Do not invent new content. Preserve meaning and sentence boundaries. \"\n",
    "    \"Return output as a JSON array of strings and nothing else.\"},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    "    temperature=0.0,\n",
    "    max_tokens=20000,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "2b9db943-ba04-4c51-b318-19f06b8cd212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14674 60466\n",
      "24.27 % of original text remained in chunked output\n"
     ]
    }
   ],
   "source": [
    "tot = 0\n",
    "for x in result:\n",
    "    tot = tot + len(x)\n",
    "\n",
    "print(tot, len(text))\n",
    "print(round(tot/len(text)*100,2), \"% of original text remained in chunked output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6834d2da-801d-4a7f-9cf5-15fa1f9529a4",
   "metadata": {},
   "source": [
    "#### Method 2: Use Non-OpenAI Chunks as Input, then Have OpenAI Enhance Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "69610566-adf4-4ee7-8106-106d02b31a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "this_doc_chunks = chunk_df.loc[chunk_df['Document']==\"EPA-HQ-OAR-2025-0194-0094_attachment_1.pdf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "f6646e65-5483-4fed-8727-e6ad0d216b1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "this_chunk_list = this_doc_chunks['Chunk'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "5ac3e0f2-e0b5-413c-82a0-61bbb86f4905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(this_chunk_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "e082b68b-4f71-457b-a962-81a6a7612d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_chunk_chars = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "8a182e6e-9bba-440a-ab31-407d97508339",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = f\"\"\"\n",
    "    \"You will be given a sequence of numbered text chunks that together form a document.\\n\\n\"\n",
    "    \"Rules (must follow exactly):\\n\"\n",
    "    \"1) Do NOT summarize, delete, reorder, or invent content. You may only split or merge contiguous text boundaries and normalize whitespace.\\n\"\n",
    "    f\"2) Aim for roughly {target_chunk_chars} characters per returned chunk, respecting sentence boundaries when possible.\\n\"\n",
    "    \"3) The returned chunks must be contiguous substrings of the concatenation of the original chunks (you may merge adjacent chunks before re-splitting).\\n\"\n",
    "    \"4) Output MUST be a single valid JSON array of strings, e.g. [\\\"chunk1\\\",\\\"chunk2\\\",...]. NOTHING else. No commentary, no metadata.\\n\\n\"\n",
    "    \"INPUT CHUNKS (do not change the labels):\\n\\n\"\n",
    "    \\\"\\\"\\\"\n",
    "    {this_chunk_list}\n",
    "    \\\"\\\"\\\"\n",
    "\n",
    "    \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "8c8bbd98-d306-43a5-912d-b31a30263db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = client.chat.completions.create(\n",
    "    model=\"GPT-4.1-nano\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You only modify chunk boundaries. Under no circumstances change the text content \"\n",
    "            \"other than whitespace normalization. Return a valid JSON array of strings and nothing else.\"\n",
    "},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    "    temperature=0.0,\n",
    "    max_tokens=20000,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "e00845a8-6e7c-4422-b9da-cfdd1635f6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = resp.choices[0].message.content.strip()\n",
    "new_chunks = json.loads(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "f50b076a-b667-43e1-82a1-9a5df507e9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57727 60466\n",
      "95.47 % of original text remained in chunked output\n"
     ]
    }
   ],
   "source": [
    "raw = resp.choices[0].message.content.strip()\n",
    "print(len(raw), len(text))\n",
    "print(round(len(raw)/len(text)*100,2), \"% of original text remained in chunked output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7b2930-8119-4687-b40e-4c115ec46fab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
